{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a704d7-3254-48a0-b37a-916fea3a216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141403e-630a-46e6-b242-cef1df8d0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "def preprocess_and_split_data(df, target_column, test_size=0.4, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the data into training, validation, and testing sets, and preprocess each set with scaling and encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The dataset to split and preprocess.\n",
    "    target_column (str): The name of the target column.\n",
    "    test_size (float): The proportion of data to include in the test + validation sets.\n",
    "    val_size (float): The proportion of the remaining data (test + validation) to allocate to validation.\n",
    "    random_state (int): Seed for random splitting.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Preprocessed training, validation, and test sets (X_train, X_val, X_test, y_train, y_val, y_test).\n",
    "    \"\"\"\n",
    "    # Step 1: Define features and split the data\n",
    "    features = df.columns.drop(target_column)\n",
    "    X = df[features]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    # Step 2: Identify categorical and numerical features\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Step 3: Define and apply transformers for each set\n",
    "    train_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "    \n",
    "    val_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "    \n",
    "    test_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "    \n",
    "    # Fit each preprocessor only on its respective set and transform\n",
    "    X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "    X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "    X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "    \n",
    "    # Display shapes of the preprocessed sets to confirm successful transformation\n",
    "    print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "    print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "    print(\"Test set shape:\", X_test_preprocessed.shape)\n",
    "    \n",
    "    return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train, y_val, y_test\n",
    "\n",
    "X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train, y_val, y_test = preprocess_and_split_data(data, target_column='churn')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
