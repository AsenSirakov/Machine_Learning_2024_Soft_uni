{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dca23e1-262f-44f9-a534-663d180a444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline  # Use imblearn's Pipeline to support SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTEENN \n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler,  InstanceHardnessThreshold\n",
    "from imblearn.under_sampling import NearMiss, ClusterCentroids, EditedNearestNeighbours, CondensedNearestNeighbour\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d58501cc-b867-474d-b832-b22861d74d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the preprocessor on the training data only\n",
    "#preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training, validation, and test sets using the same preprocessor\n",
    "\n",
    "#X_train_preprocessed = preprocessor.transform(X_train)\n",
    "#X_val_preprocessed = preprocessor.transform(X_val)\n",
    "#X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# Output the shapes of the processed datasets to confirm transformation\n",
    "\n",
    "#print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "#print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "#print(\"Test set shape:\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a830987-30cb-4787-9950-3938192a3ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_data_telecom.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee2a1cfe-9dce-4e2e-bf18-17a60dc6a573",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = data.drop(['total_charges'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e9bf97-cc01-4c5b-9cb6-b5ae163c0824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e59ea96f-ea30-422e-baaa-cd1b7d93e059",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Female'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_124468\\3216674927.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;31m#best_model = grid_search.best_estimator_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;31m#y_val_pred = best_model.predict(X_val)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# Predict on validation set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                 \u001b[0mlast_step_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"fit\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1471\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1472\u001b[0m                 \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1474\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    191\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1259\u001b[0m         raise ValueError(\n\u001b[0;32m   1260\u001b[0m             \u001b[1;33mf\"\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mestimator_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m requires y to be passed, but the target y is None\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1263\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1264\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    994\u001b[0m                         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    997\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 998\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    999\u001b[0m                 raise ValueError(\n\u001b[0;32m   1000\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m                 \u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m   2149\u001b[0m     def __array__(\n\u001b[0;32m   2150\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2151\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2153\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2154\u001b[0m         if (\n\u001b[0;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2156\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Female'"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline  # Use imblearn's Pipeline to support SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "#models = {\n",
    "   # 'Random Forest': RandomForestClassifier(n_estimators=100, random_state=50),\n",
    "   # 'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=50),\n",
    "    #'Logistic Regression': LogisticRegression(max_iter=1000, random_state=50),\n",
    "   # 'SVM': SVC(kernel='linear', probability=True, random_state=50)\n",
    "#}\n",
    "\n",
    "# Assuming `data_no_total` is your DataFrame and `churn` is the target column\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Define categorical and numerical features\n",
    "# categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "# numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define the ColumnTransformer for preprocessing\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numerical_features),\n",
    "#         ('cat', OneHotEncoder(drop= 'first'), categorical_features)\n",
    "#     ]\n",
    "# )\n",
    "# Create a pipeline with preprocessing, SMOTE, feature selection, and the model\n",
    "pipeline = Pipeline([\n",
    "    # ('preprocessor', preprocessor),\n",
    "    # ('tomek', SMOTE()),\n",
    "    # #('tomek2',TomekLinks()),\n",
    "    # ('smoteeen', RandomUnderSampler()),\n",
    "    ('classifier', SVC(kernel='linear', probability=True, random_state=42)),\n",
    " \n",
    "])\n",
    "\n",
    "#aram_grid = {\n",
    "    #'classifier__n_estimators': [100, 200, 300],\n",
    "    #'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    #'classifier__max_depth': [3, 5, 7],\n",
    "    #'classifier__subsample': [0.8, 1.0]  # Fewer values to test here\n",
    "#}\n",
    "\n",
    "\n",
    "# Set up GridSearchCV\n",
    "#grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search on training data\n",
    "#grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best cross-validation score\n",
    "#print(\"Best parameters found: \", grid_search.best_params_)\n",
    "#print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "# Use the best model to make predictions on the validation set\n",
    "#best_model = grid_search.best_estimator_\n",
    "#y_val_pred = best_model.predict(X_val)\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb93803-e7cc-4ab0-a0e4-36bd349b0bba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "061d6ce6-5c87-49f1-a97f-3b3585b84189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 82.24%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.92      0.88      1037\n",
      "         Yes       0.70      0.55      0.62       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.78      0.73      0.75      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # Assuming `data_no_total` is your DataFrame and `churn` is the target column\n",
    "# features = data_no_total.columns.drop('churn')\n",
    "# target = 'churn'\n",
    "\n",
    "# # Split the data into training, validation, and test sets\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Define categorical and numerical features\n",
    "# categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "# numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# # Define the ColumnTransformer for preprocessing\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', StandardScaler(), numerical_features),\n",
    "#         ('cat', OneHotEncoder(drop= 'first'), categorical_features)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # # Define base estimators\n",
    "# # estimators = [\n",
    "# #     ('lr', LogisticRegression(max_iter=1000, random_state=42)),\n",
    "# #     ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "# #     ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "# # ]\n",
    "\n",
    "# # # Meta-model\n",
    "# # stacking_clf = StackingClassifier(\n",
    "# #     estimators=estimators,\n",
    "# #     final_estimator= SVC(kernel='linear', probability=True, random_state=42),\n",
    "# #     cv=5\n",
    "# # )\n",
    "\n",
    "# # Pipeline with stacking classifier\n",
    "# pipeline1 = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     #('tomek', SMOTE()),\n",
    "#     #('tomek2',TomekLinks()),\n",
    "#     #('smoteeen', SMOTEENN()),\n",
    "#     ('stacking_classifier', stacking_clf)\n",
    "# ])\n",
    "\n",
    "# # # Train and predict\n",
    "# # pipeline1.fit(X_train, y_train)\n",
    "# # y_val_pred = pipeline1.predict(X_val)\n",
    "# # accuracy = accuracy_score(y_val, y_val_pred)\n",
    "# # print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # # Display classification report\n",
    "# # print(\"\\nClassification Report:\")\n",
    "# # print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e3b8ae-d9a4-4aa2-9358-dd8f611465be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['gender', 'senior_citizen', 'partner', 'dependents', 'tenure',\n",
       "       'phone_service', 'multiple_lines', 'internet_service',\n",
       "       'online_security', 'online_backup', 'device_protection', 'tech_support',\n",
       "       'streaming_t_v', 'streaming_movies', 'contract', 'paperless_billing',\n",
       "       'payment_method', 'monthly_charges'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c746d-0020-46da-af6d-ef68438b8b93",
   "metadata": {},
   "source": [
    "### Pipeline: Explanation\n",
    "16. Now let's first explain what a pipeline is: A pipeline in machine learning is a tool for automating a sequence of data transformation and model-building steps. It provides a structured way to define, execute, and reproduce an end-to-end workflow, from data preprocessing to model fitting and evaluation, ensuring that each step is applied consistently and in the right order. Pipelines are especially useful in complex workflows involving multiple preprocessing, transformation, and modeling steps.\n",
    "17. Key components of a pipeline:\n",
    "- **Transformers: These are steps that apply transformations to the data, such as:**\n",
    "- Scaling: Adjusts numerical data to a common scale, often with StandardScaler (normalizes data to have mean 0 and variance 1) or MinMaxScaler (scales data to a [0, 1] range).\n",
    "- Encoding: Converts categorical data into numerical format, such as one-hot encoding, which is necessary for algorithms that require numerical inputs.\n",
    "- Imputation: Handles missing values by replacing them with statistical values (like mean, median) or other strategies.\n",
    "- Feature Selection: Reduces the number of input features by selecting the most relevant ones.\n",
    "- **Estimator: This is the machine learning model that will learn from the preprocessed data, such as:**\n",
    "- Linear models: Logistic regression, linear regression.\n",
    "- Tree-based models: Decision trees, random forests, gradient boosting.\n",
    "- Support Vector Machines and other classifiers or regressors.\n",
    "- **Feature selection or data resampling steps (could be done)**\n",
    "- Resampling: Handle imbalances in the dataset, like oversampling with SMOTE.\n",
    "- Feature selection : Select or reduce features based on their importance or correlation.\n",
    "\n",
    "18. Fitting the Pipeline: When fit is called on the pipeline, it sequentially applies each transformation step to the data before passing it to the estimator.\n",
    "- For example, a pipeline with scaling, encoding, and a classifier will first scale and encode the data before passing it to the classifier to train.\n",
    "- Predicting with the Pipeline: When predict is called, the pipeline again applies the transformations to the new data in the same order before passing it to the trained estimator for predictions.\n",
    "18. Pros and cons for pipelines:\n",
    "- **Benefits of Pipelines**\n",
    "- Consistency and Reproducibility: All transformations are consistently applied in the same order, ensuring that the validation and test sets are processed the same way as the training set. It’s easier to reproduce results and document each step.\n",
    "- Avoiding Data Leakage: By separating training-only steps,pipelines reduce the chance of data leakage. For example, when scaling, the StandardScaler only learns the mean and standard deviation from the training set, ensuring that the validation/test sets remain untouched.\n",
    "- Simplifying Code: Rather than writing repetitive code, pipelines consolidate it into one workflow, making the code cleaner and easier to manage.\n",
    "- Automating Cross-Validation: Pipelines can be used with techniques like cross-validation, allowing you to tune and evaluate models without having to handle data preprocessing manually for each fold.\n",
    "- Flexibility with Experimentation: You can easily swap or add steps. For example, changing from SMOTE to a different resampling method (like ADASYN) or from Random Forest to Gradient Boosting without having to reapply preprocessing manually.\n",
    "- **Cons of Pipelines**\n",
    "- Inflexibility for Complex Workflows: Pipelines in libraries like scikit-learn are typically linear, meaning each step must proceed sequentially. For complex workflows that require parallel processing or conditional logic (e.g., using different preprocessing steps for different subsets of data), standard pipelines can be limiting.\n",
    "- Debugging Challenges: When all steps are bundled into a single pipeline, tracing and debugging issues within individual steps becomes harder. For instance, if a specific transformer is failing, it may be less straightforward to isolate and fix the problem within a pipeline.\n",
    "- Limited Control Over Intermediate Outputs: Pipelines are typically designed to pass data sequentially without allowing access to intermediate outputs. If you need to inspect the transformed data at various stages, it’s not directly possible in a scikit-learn pipeline.\n",
    "- Not Always Ideal for Experimentation: In exploratory phases, pipelines may restrict flexibility since they standardize preprocessing and modeling into a rigid structure. When testing many models or trying various feature engineering steps, pipelines can feel less agile.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd02e32-71c9-4a61-a5be-9d9eb222aaaf",
   "metadata": {},
   "source": [
    "### Pipeline : Creating and working with it \n",
    "20. Now let's create the pipeline including only the preprocessor for now just to show what exactly is more simply and straightforward than manually doing it.\n",
    "- Pipeline is a class in scikit-learn that allows you to chain together multiple steps, like preprocessing and model training, in a sequential and organized way. The steps in the pipeline are executed in the order they’re listed. Each step in the pipeline consists of:\n",
    "- A name (like 'preprocessor' or 'classifier') — this is an identifier for the step and can be any unique string.\n",
    "- An operation — this is usually a transformation or a model, like a scaler, encoder, or classifier.\n",
    "\n",
    "21. Think of it as a list of tasks that need to be done in a certain order to prepare your data and fit a model. Each task (or step) is a tuple, which contains two parts:\n",
    "- A name for the step — this is just a string you can choose, like 'preprocessor' or 'classifier'.\n",
    "- The operation or action that step performs — this could be a transformer (like scaling or encoding) or a model (like SVC for classification).\n",
    "\n",
    "22. We will now declare a variable in which we will save a pipeline (list of operations) that for now will contain only a preprocessor.\n",
    "- We will declare the name 'preprocessor' that will be doing the operation we created earlier where we defined the preprocessor for scaling and encoding. I do that as I have already declared a functional preprocessor and why not use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054a23c-363c-44ed-a2fe-d38557186330",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_example = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc29c1-5fb0-414b-b0b0-3a7ac94d3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e11d4d-c6cd-41f0-85c4-91bdd0afbbb7",
   "metadata": {},
   "source": [
    "23. Now this doesn't speak much does it, it looks simple and straightforward. We see a graph that states that there is preprocessor and in this preprocessor we have both the scaler and the encoder. Behind that though the preporcessor is fit to the training set without explicitly doing it and most importantly, when later we are gonna predict on the validation and test sets(test set in the end). The pipeline automatically applies the transformation from the training set to the other one which we are predicting on. This makes the process extremely straightforward and clean without the need of us explicitly fitting the transformer to the other sets. It automatically fits the transformation only on the training set \n",
    "- When working with separate train, validation, and test sets, consistency is key. By using a pipeline, the exact same preprocessing transformations (e.g., scaling and encoding) are applied to each dataset split. This ensures that the model sees data in the same format during training and evaluation, reducing the risk of data leakage or inconsistent transformations.\n",
    "- The pipeline automatically applies each step in sequence. You specify each transformation in the order you want, and the pipeline handles fitting and transforming the data step-by-step, simplifying your code and reducing human error.\n",
    "- Since the pipeline follows a defined structure, it prevents common errors like forgetting to apply the scaler or encoder to new data. Once set up, the pipeline will consistently follow the same sequence every time, which reduces the chance of skipping steps or applying them incorrectly.\n",
    "- Using a pipeline makes the code more readable by abstracting away repetitive tasks. Each transformation step is encapsulated, so you can quickly understand what each part of the code does without getting bogged down in details.\n",
    "\n",
    "24. I stated many things but haven't showed proof and how exactly it works so let's proceed.\n",
    "25. Now we have our 'transformer' in the face of the preprocessor, let's now add the 'estimator'.\n",
    "- I will try with a Random Forest classifier first.\n",
    "- What I am going to do is just add another tuple in the pipeline with name 'classifier' and next to it the model : RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "- It's going to look like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2ebf7-116b-4c8a-a705-036551b61e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_example = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7f54a-4ef0-4304-9764-56640e16123e",
   "metadata": {},
   "source": [
    "23. As we can see here we added a classification model to the pipeline pretty easy. All we did is add the tuple in the list, now the pipeline is done but it's not fitted. So we shall now fit it and try to gain some results out of it.\n",
    "24. Now Pipeline fitting :Training the Pipeline (pipeline.fit(X_train, y_train)):\n",
    "- The pipeline first applies the preprocessor step on X_train, transforming the data (e.g., scaling, encoding).\n",
    "- The transformed data is then passed to the classifier step, which trains the Random Forest model on the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e318ebb-3c9a-4b22-96a5-ae0e823502df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_example.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ae0238-14f0-42ed-b136-2e6cd8d9ee29",
   "metadata": {},
   "source": [
    "25. As we can now see the pipeline is fitted, the graph we see is now blue which indicates that it's fittend and that can also be seen on the top right corner of the graph there is and 'i' symbol which shows the current state of the pipeline(fiited or not fitted).\n",
    "26. Now all that is left is to do a prediction\n",
    "- When predicting, the pipeline applies the same scaling and encoding transformations to new data before making predictions with the trained model.\n",
    "- The predict phase of a pipeline in scikit-learn is where the trained pipeline takes in new data and outputs predictions.\n",
    "- Let's do the prediction and store the result in a variable y_val_predict_example (This saves the result of the prediction on the X_val). We do that so that we can use the variable for (scikit learn's metrics) classification reports and accuracy evaluations(accuracy_score) and feature importances.\n",
    "- As pipelines are mainly for transforming, fitting and predicting. The metrics we get from other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77acdc3-32f6-46be-a42a-5cabf6e6cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_example = pipeline_example.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78866bcb-16b8-4abf-b523-d4b35cd0381a",
   "metadata": {},
   "source": [
    "27. When we call pipeline.predict(X_val), the following happens:\n",
    "- X_val is the new data (usually the validation or test set) for which we want predictions.\n",
    "- The pipeline will process this data through all the transformations defined in its steps, ending with the model making predictions based on the processed data\n",
    "\n",
    "28. And that's all of it it's pretty straightforward and easy to work with it looks clean(in my case not, because I split it all and give a lot of exlpanations in between). But now if i want to exchange the model i just edit the part of the tuple next to the 'classificator' with the model i want and i have another classifying model that gets the transformed data. All of this is consistant and no data is leaked and every step is secured. In our case this is good as we are going to try multiple models and try different approaches and tunings in order to get the best result(best as of combination of relism and predicting power).\n",
    "\n",
    "29. As I stated earlier these steps that i will now show were already done manually in another notebook, and I can say that it was much harder as the workflow isn't straightforward for every model a new code block, new variables that could be forgotten, really messy workflow that I could somehow keep the track of as the data ain't much and i had a lot of time, but in real world scenario this would be extremely tiring and confusing. And the fact remains that still somewhere in the process I could've leaked data which is bad and using pipelines this is really hard to happen. So that is why I will be proceeding with pipelines even though it could be a little bit limiting for some things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0c61fd6-24ff-47dc-b814-30916c6aca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: Random Forest\n",
      "Validation Accuracy: 79.24%\n",
      "Classification Report:\n",
      "{'No': {'precision': 0.8198970840480274, 'recall': 0.9218900675024108, 'f1-score': 0.8679073990013618, 'support': 1037.0}, 'Yes': {'precision': 0.6567796610169492, 'recall': 0.4246575342465753, 'f1-score': 0.5158069883527454, 'support': 365.0}, 'accuracy': 0.7924393723252496, 'macro avg': {'precision': 0.7383383725324884, 'recall': 0.6732738008744931, 'f1-score': 0.6918571936770537, 'support': 1402.0}, 'weighted avg': {'precision': 0.7774307078666126, 'recall': 0.7924393723252496, 'f1-score': 0.7762407443032554, 'support': 1402.0}}\n",
      "\n",
      "Model: Gradient Boosting\n",
      "Validation Accuracy: 81.67%\n",
      "Classification Report:\n",
      "{'No': {'precision': 0.8451327433628318, 'recall': 0.9209257473481196, 'f1-score': 0.8814028610982926, 'support': 1037.0}, 'Yes': {'precision': 0.6985294117647058, 'recall': 0.5205479452054794, 'f1-score': 0.5965463108320251, 'support': 365.0}, 'accuracy': 0.8166904422253923, 'macro avg': {'precision': 0.7718310775637689, 'recall': 0.7207368462767996, 'f1-score': 0.7389745859651589, 'support': 1402.0}, 'weighted avg': {'precision': 0.8069656848511941, 'recall': 0.8166904422253923, 'f1-score': 0.807242632248658, 'support': 1402.0}}\n",
      "\n",
      "Model: Logistic Regression\n",
      "Validation Accuracy: 82.03%\n",
      "Classification Report:\n",
      "{'No': {'precision': 0.8452066842568162, 'recall': 0.926711668273867, 'f1-score': 0.8840846366145354, 'support': 1037.0}, 'Yes': {'precision': 0.7132075471698113, 'recall': 0.5178082191780822, 'f1-score': 0.6, 'support': 365.0}, 'accuracy': 0.8202567760342369, 'macro avg': {'precision': 0.7792071157133138, 'recall': 0.7222599437259746, 'f1-score': 0.7420423183072677, 'support': 1402.0}, 'weighted avg': {'precision': 0.810841716327603, 'recall': 0.8202567760342369, 'f1-score': 0.8101253695929196, 'support': 1402.0}}\n",
      "\n",
      "Model: SVM\n",
      "Validation Accuracy: 81.88%\n",
      "Classification Report:\n",
      "{'No': {'precision': 0.8517520215633423, 'recall': 0.914175506268081, 'f1-score': 0.881860465116279, 'support': 1037.0}, 'Yes': {'precision': 0.6920415224913494, 'recall': 0.547945205479452, 'f1-score': 0.6116207951070336, 'support': 365.0}, 'accuracy': 0.818830242510699, 'macro avg': {'precision': 0.7718967720273459, 'recall': 0.7310603558737665, 'f1-score': 0.7467406301116564, 'support': 1402.0}, 'weighted avg': {'precision': 0.8101726120331871, 'recall': 0.818830242510699, 'f1-score': 0.8115056294861972, 'support': 1402.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define the ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store performance results\n",
    "results = {}\n",
    "\n",
    "# Loop through models, applying each to the pipeline\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate on validation data\n",
    "    y_val_pred = pipeline.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    results[model_name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "# Display results for each model\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(f\"Validation Accuracy: {metrics['accuracy'] * 100:.2f}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(metrics['classification_report'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c8021-816e-48ed-927e-2ee4f1417ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb4407-bebd-4cbd-b4dd-5cbab8458f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define the model and pipeline\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Initialize a dictionary to store metrics\n",
    "results = {}\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Calculate validation accuracy\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "results['Validation Accuracy'] = accuracy * 100\n",
    "\n",
    "# Display classification report and save it\n",
    "classification_rep = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "results['Classification Report'] = classification_rep\n",
    "\n",
    "# If the model supports feature importances, save them\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    feature_importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    results['Feature Importances'] = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Validation Accuracy: {results['Validation Accuracy']:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "if 'Feature Importances' in results:\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(results['Feature Importances'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b1e277-df7d-46d4-bdf5-b3c79af5f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(kernel='linear', probability=True, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Meta model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=meta_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08711285-e8a3-42f3-a8ba-5755d91f7e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "   ('preprocessor', preprocessor),\n",
    "    ('stacking', stacking_clf),\n",
    "    \n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_val_pred = pipeline.predict(X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
