{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21623389-664a-4826-ada9-14947c358d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f87ae0b-9569-453e-8dac-d5efb63ebf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_data_telecom.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b72376-ccf2-4c41-b6a6-5b4c54563a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = data.drop(['total_charges'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab0c0d9-e6e9-41ff-ae4c-c69bacbfddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4206, 29)\n",
      "Validation set shape: (1402, 29)\n",
      "Test set shape: (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume `data` is the DataFrame and 'contract' is the feature where we want 'Month-to-month' to remain\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Separate out 'contract' and other categorical/numerical features\n",
    "contract_feature = ['contract']\n",
    "other_categorical_features = [col for col in X_train.select_dtypes(include=['object']).columns if col != 'contract']\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define separate transformers for contract and other categorical features\n",
    "contract_transformer = OneHotEncoder(drop=['Two year'], sparse_output=False)  # Dropping 'Two year' to keep 'Month-to-month'\n",
    "other_categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)  # Default drop first for other categories\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the datasets\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Output the shapes of the processed datasets\n",
    "print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape:\", X_test_preprocessed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572a11a1-40d2-4868-a1b1-027dba3c26fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4206, 29)\n",
      "Validation set shape: (1402, 29)\n",
      "Test set shape: (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define a preprocessor that will be fit only on the training data\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data only\n",
    "train_preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training, validation, and test sets using the same preprocessor\n",
    "X_train_preprocessed = train_preprocessor.transform(X_train)\n",
    "X_val_preprocessed = train_preprocessor.transform(X_val)\n",
    "X_test_preprocessed = train_preprocessor.transform(X_test)\n",
    "\n",
    "# Output the shapes of the processed datasets to confirm transformation\n",
    "print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape:\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598bce64-ad30-4c6e-b837-df7ad1aa0c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape after SMOTE: (6182, 29)\n",
      "Validation set shape (untouched): (1402, 29)\n",
      "Test set shape (untouched): (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Step 1: Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define a preprocessor for scaling and encoding\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data only\n",
    "train_preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training set, then apply SMOTE\n",
    "X_train_preprocessed = train_preprocessor.transform(X_train)\n",
    "X_val_preprocessed = train_preprocessor.transform(X_val)\n",
    "X_test_preprocessed = train_preprocessor.transform(X_test)\n",
    "\n",
    "# Step 4: Apply SMOTE only on the preprocessed training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# Output the shapes to confirm transformations\n",
    "print(\"Training set shape after SMOTE:\", X_train_resampled.shape)\n",
    "print(\"Validation set shape (untouched):\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape (untouched):\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3271d72a-1b48-4910-9d16-7867823ac766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 79.24%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.92      0.87      1037\n",
      "         Yes       0.66      0.42      0.52       365\n",
      "\n",
      "    accuracy                           0.79      1402\n",
      "   macro avg       0.74      0.67      0.69      1402\n",
      "weighted avg       0.78      0.79      0.78      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 81.88%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.91      0.88      1037\n",
      "         Yes       0.69      0.55      0.61       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.77      0.73      0.75      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 82.03%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.93      0.88      1037\n",
      "         Yes       0.71      0.52      0.60       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.78      0.72      0.74      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 71.47%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.79      0.80      1037\n",
      "         Yes       0.46      0.49      0.47       365\n",
      "\n",
      "    accuracy                           0.71      1402\n",
      "   macro avg       0.64      0.64      0.64      1402\n",
      "weighted avg       0.72      0.71      0.72      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 81.67%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.92      0.88      1037\n",
      "         Yes       0.70      0.52      0.60       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.77      0.72      0.74      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7924393723252496, 'SVM': 0.818830242510699, 'Logistic Regression': 0.8202567760342369, 'Decision Tree': 0.7146932952924394, 'Gradient Boosting': 0.8166904422253923}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.023080       0.012882   \n",
      "num__tenure                                       0.244376       0.271064   \n",
      "num__monthly_charges                              0.217409       0.289251   \n",
      "cat__gender_Male                                  0.036319       0.022723   \n",
      "cat__partner_Yes                                  0.027956       0.024837   \n",
      "cat__dependents_Yes                               0.023801       0.028811   \n",
      "cat__phone_service_Yes                            0.005704       0.005351   \n",
      "cat__multiple_lines_No phone service              0.005793       0.000000   \n",
      "cat__multiple_lines_Yes                           0.025386       0.012333   \n",
      "cat__internet_service_Fiber optic                 0.041959       0.094146   \n",
      "cat__internet_service_No                          0.007710       0.000000   \n",
      "cat__online_security_No internet service          0.003548       0.000000   \n",
      "cat__online_security_Yes                          0.027384       0.017994   \n",
      "cat__online_backup_No internet service            0.003788       0.000000   \n",
      "cat__online_backup_Yes                            0.026621       0.021473   \n",
      "cat__device_protection_No internet service        0.006713       0.000000   \n",
      "cat__device_protection_Yes                        0.022984       0.020854   \n",
      "cat__tech_support_No internet service             0.003243       0.000000   \n",
      "cat__tech_support_Yes                             0.027853       0.017744   \n",
      "cat__streaming_t_v_No internet service            0.005644       0.000000   \n",
      "cat__streaming_t_v_Yes                            0.020891       0.015072   \n",
      "cat__streaming_movies_No internet service         0.006406       0.014946   \n",
      "cat__streaming_movies_Yes                         0.021968       0.010265   \n",
      "cat__contract_One year                            0.028882       0.022213   \n",
      "cat__contract_Two year                            0.036878       0.010236   \n",
      "cat__paperless_billing_Yes                        0.031230       0.024837   \n",
      "cat__payment_method_Credit card (automatic)       0.017267       0.018066   \n",
      "cat__payment_method_Electronic check              0.033031       0.027678   \n",
      "cat__payment_method_Mailed check                  0.016176       0.017223   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.006910  \n",
      "num__tenure                                           0.364810  \n",
      "num__monthly_charges                                  0.098273  \n",
      "cat__gender_Male                                      0.001321  \n",
      "cat__partner_Yes                                      0.001196  \n",
      "cat__dependents_Yes                                   0.003787  \n",
      "cat__phone_service_Yes                                0.009364  \n",
      "cat__multiple_lines_No phone service                  0.004518  \n",
      "cat__multiple_lines_Yes                               0.012862  \n",
      "cat__internet_service_Fiber optic                     0.183115  \n",
      "cat__internet_service_No                              0.002183  \n",
      "cat__online_security_No internet service              0.010706  \n",
      "cat__online_security_Yes                              0.024364  \n",
      "cat__online_backup_No internet service                0.010075  \n",
      "cat__online_backup_Yes                                0.001057  \n",
      "cat__device_protection_No internet service            0.001187  \n",
      "cat__device_protection_Yes                            0.000000  \n",
      "cat__tech_support_No internet service                 0.001445  \n",
      "cat__tech_support_Yes                                 0.007300  \n",
      "cat__streaming_t_v_No internet service                0.001801  \n",
      "cat__streaming_t_v_Yes                                0.005863  \n",
      "cat__streaming_movies_No internet service             0.022903  \n",
      "cat__streaming_movies_Yes                             0.010181  \n",
      "cat__contract_One year                                0.062484  \n",
      "cat__contract_Two year                                0.078849  \n",
      "cat__paperless_billing_Yes                            0.025229  \n",
      "cat__payment_method_Credit card (automatic)           0.005076  \n",
      "cat__payment_method_Electronic check                  0.042154  \n",
      "cat__payment_method_Mailed check                      0.000987  \n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    model.fit(X_train_preprocessed, y_train)\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284bbed7-efe0-4309-a420-c61e5fe9f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected features, based on their transformed names\n",
    "selected_feature_names = [\n",
    "    'num__tenure', \n",
    "    'num__monthly_charges', \n",
    "    'cat__internet_service_Fiber optic', \n",
    "    'contract__contract_Month-to-month', \n",
    "    'cat__contract_One year', \n",
    "    'cat__paperless_billing_Yes', \n",
    "    'cat__payment_method_Electronic check', \n",
    "    'cat__online_security_Yes', \n",
    "    'cat__dependents_Yes', \n",
    "    'cat__payment_method_Credit card (automatic)',\n",
    "    'cat__gender_Male',\n",
    "    'cat__partner_Yes',\n",
    "    'cat__tech_support_Yes',\n",
    "    'cat__online_backup_Yes',\n",
    "    'cat__streaming_movies_Yes'\n",
    "    \n",
    "]\n",
    "\n",
    "# Get all transformed feature names from the preprocessor\n",
    "all_feature_names = train_preprocessor.get_feature_names_out()\n",
    "\n",
    "# Find the indices of selected features in the preprocessed dataset\n",
    "selected_feature_indices = [i for i, feature in enumerate(all_feature_names) if feature in selected_feature_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f6a2a3-20a4-4167-ada7-7fc23d98b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training set shape: (4206, 14)\n",
      "Filtered Validation set shape: (1402, 14)\n",
      "Filtered Test set shape: (1402, 14)\n"
     ]
    }
   ],
   "source": [
    "# Filter each preprocessed dataset to keep only selected features\n",
    "X_train_selected = X_train_preprocessed[:, selected_feature_indices]\n",
    "X_val_selected = X_val_preprocessed[:, selected_feature_indices]\n",
    "X_test_selected = X_test_preprocessed[:, selected_feature_indices]\n",
    "\n",
    "# Check shapes to confirm\n",
    "print(\"Filtered Training set shape:\", X_train_selected.shape)\n",
    "print(\"Filtered Validation set shape:\", X_val_selected.shape)\n",
    "print(\"Filtered Test set shape:\", X_test_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886601e6-0a02-42cc-8e7a-8d87da8a696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest with selected features...\n",
      "Random Forest Validation Accuracy with selected features: 77.60%\n",
      "\n",
      "Random Forest Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.89      0.86      1037\n",
      "         Yes       0.59      0.44      0.50       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.71      0.67      0.68      1402\n",
      "weighted avg       0.76      0.78      0.76      1402\n",
      "\n",
      "Training and evaluating SVM with selected features...\n",
      "SVM Validation Accuracy with selected features: 80.74%\n",
      "\n",
      "SVM Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.91      0.88      1037\n",
      "         Yes       0.67      0.51      0.58       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.71      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "Training and evaluating Logistic Regression with selected features...\n",
      "Logistic Regression Validation Accuracy with selected features: 81.03%\n",
      "\n",
      "Logistic Regression Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.92      0.88      1037\n",
      "         Yes       0.69      0.50      0.58       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.71      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "Training and evaluating Decision Tree with selected features...\n",
      "Decision Tree Validation Accuracy with selected features: 71.54%\n",
      "\n",
      "Decision Tree Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.81      0.81      0.81      1037\n",
      "         Yes       0.45      0.45      0.45       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.63      0.63      0.63      1402\n",
      "weighted avg       0.72      0.72      0.72      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting with selected features...\n",
      "Gradient Boosting Validation Accuracy with selected features: 81.24%\n",
      "\n",
      "Gradient Boosting Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.92      0.88      1037\n",
      "         Yes       0.68      0.52      0.59       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.72      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "\n",
      "Model Performance Summary with Selected Features:\n",
      "{'Random Forest': 0.776034236804565, 'SVM': 0.8074179743223966, 'Logistic Regression': 0.8102710413694721, 'Decision Tree': 0.7154065620542083, 'Gradient Boosting': 0.8124108416547788}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store model performance\n",
    "model_performance_selected = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name} with selected features...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = model.predict(X_val_selected)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    model_performance_selected[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy with selected features: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report with selected features:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nModel Performance Summary with Selected Features:\")\n",
    "print(model_performance_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4105fd3e-bbde-49b7-9590-e228d8ede6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [3091 3091]\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 78.60%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      1037\n",
      "           1       0.59      0.56      0.58       365\n",
      "\n",
      "    accuracy                           0.79      1402\n",
      "   macro avg       0.72      0.71      0.72      1402\n",
      "weighted avg       0.78      0.79      0.78      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 74.61%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1037\n",
      "           1       0.51      0.75      0.61       365\n",
      "\n",
      "    accuracy                           0.75      1402\n",
      "   macro avg       0.70      0.75      0.71      1402\n",
      "weighted avg       0.79      0.75      0.76      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 72.47%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81      1037\n",
      "           1       0.48      0.55      0.51       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.65      0.67      0.66      1402\n",
      "weighted avg       0.74      0.72      0.73      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 78.03%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84      1037\n",
      "           1       0.56      0.72      0.63       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.73      0.76      0.74      1402\n",
      "weighted avg       0.80      0.78      0.79      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7860199714693296, 'SVM': 0.6961483594864479, 'Logistic Regression': 0.746077032810271, 'Decision Tree': 0.724679029957204, 'Gradient Boosting': 0.7803138373751783}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.017579       0.015295   \n",
      "num__tenure                                       0.197041       0.164662   \n",
      "num__monthly_charges                              0.156413       0.190125   \n",
      "cat__gender_Male                                  0.032803       0.039956   \n",
      "cat__partner_Yes                                  0.030093       0.021467   \n",
      "cat__dependents_Yes                               0.029341       0.024592   \n",
      "cat__phone_service_Yes                            0.005098       0.010648   \n",
      "cat__multiple_lines_No phone service              0.005417       0.000595   \n",
      "cat__multiple_lines_Yes                           0.024251       0.012532   \n",
      "cat__internet_service_Fiber optic                 0.039615       0.034586   \n",
      "cat__internet_service_No                          0.011168       0.000000   \n",
      "cat__online_security_No internet service          0.005086       0.000000   \n",
      "cat__online_security_Yes                          0.044685       0.025026   \n",
      "cat__online_backup_No internet service            0.006306       0.008762   \n",
      "cat__online_backup_Yes                            0.028862       0.020298   \n",
      "cat__device_protection_No internet service        0.008231       0.000000   \n",
      "cat__device_protection_Yes                        0.022927       0.015997   \n",
      "cat__tech_support_No internet service             0.003943       0.000000   \n",
      "cat__tech_support_Yes                             0.035304       0.017560   \n",
      "cat__streaming_t_v_No internet service            0.007342       0.000000   \n",
      "cat__streaming_t_v_Yes                            0.018876       0.011706   \n",
      "cat__streaming_movies_No internet service         0.008734       0.000000   \n",
      "cat__streaming_movies_Yes                         0.020469       0.021235   \n",
      "cat__contract_One year                            0.043453       0.136082   \n",
      "cat__contract_Two year                            0.069513       0.162832   \n",
      "cat__paperless_billing_Yes                        0.040523       0.024571   \n",
      "cat__payment_method_Credit card (automatic)       0.018838       0.009357   \n",
      "cat__payment_method_Electronic check              0.052554       0.019930   \n",
      "cat__payment_method_Mailed check                  0.015534       0.012187   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.001933  \n",
      "num__tenure                                           0.198356  \n",
      "num__monthly_charges                                  0.025857  \n",
      "cat__gender_Male                                      0.013192  \n",
      "cat__partner_Yes                                      0.009759  \n",
      "cat__dependents_Yes                                   0.039352  \n",
      "cat__phone_service_Yes                                0.004921  \n",
      "cat__multiple_lines_No phone service                  0.003072  \n",
      "cat__multiple_lines_Yes                               0.027533  \n",
      "cat__internet_service_Fiber optic                     0.084694  \n",
      "cat__internet_service_No                              0.013317  \n",
      "cat__online_security_No internet service              0.011206  \n",
      "cat__online_security_Yes                              0.049323  \n",
      "cat__online_backup_No internet service                0.006806  \n",
      "cat__online_backup_Yes                                0.017771  \n",
      "cat__device_protection_No internet service            0.002864  \n",
      "cat__device_protection_Yes                            0.007188  \n",
      "cat__tech_support_No internet service                 0.000000  \n",
      "cat__tech_support_Yes                                 0.016499  \n",
      "cat__streaming_t_v_No internet service                0.000297  \n",
      "cat__streaming_t_v_Yes                                0.013831  \n",
      "cat__streaming_movies_No internet service             0.005347  \n",
      "cat__streaming_movies_Yes                             0.018112  \n",
      "cat__contract_One year                                0.130855  \n",
      "cat__contract_Two year                                0.167858  \n",
      "cat__paperless_billing_Yes                            0.051184  \n",
      "cat__payment_method_Credit card (automatic)           0.011216  \n",
      "cat__payment_method_Electronic check                  0.061826  \n",
      "cat__payment_method_Mailed check                      0.005831  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda64be2-3924-4a09-a70f-ad2bc5f88568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with threshold 0.3: 71.61%\n",
      "Classification Report with Custom Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      1037\n",
      "           1       0.47      0.75      0.58       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.68      0.73      0.68      1402\n",
      "weighted avg       0.78      0.72      0.73      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, accuracy_score\n",
    "\n",
    "# Fit the RandomForest model on resampled training data\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predicted probabilities on the validation set\n",
    "y_val_probs = rf_model.predict_proba(X_val_preprocessed)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "# Determine optimal threshold using the precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val_encoded, y_val_probs)\n",
    "# Set a custom threshold, e.g., 0.3, to increase recall\n",
    "custom_threshold = 0.3\n",
    "y_val_pred_custom = (y_val_probs >= custom_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the custom threshold\n",
    "print(f\"Validation Accuracy with threshold {custom_threshold}: {accuracy_score(y_val_encoded, y_val_pred_custom) * 100:.2f}%\")\n",
    "print(\"Classification Report with Custom Threshold:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9be3d2-78f0-417f-b575-b415c97f35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10, 'class_weight': 'balanced_subsample'}\n",
      "Best Recall Score from Cross-Validation: 0.8622307182833772\n",
      "Validation Classification Report with Tuned Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.64      0.76      1037\n",
      "           1       0.46      0.86      0.60       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.69      0.75      0.68      1402\n",
      "weighted avg       0.81      0.70      0.72      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for Random Forest with cross-validation\n",
    "grid_search_rf = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=20,\n",
    "    scoring='recall',  # Focusing on recall for the churn class\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search on the resampled training data\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters and best recall score\n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best Recall Score from Cross-Validation:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Evaluate best model on validation set with custom threshold if needed\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_val_probs_rf = best_rf_model.predict_proba(X_val_preprocessed)[:, 1]\n",
    "y_val_pred_rf_custom = (y_val_probs_rf >= custom_threshold).astype(int)  # Using the same custom threshold from above\n",
    "\n",
    "print(\"Validation Classification Report with Tuned Random Forest:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_rf_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc317f3-4b39-4e74-bf3f-8b3db50b1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 70.9700427960057\n",
      "Ensemble Classification Report with Custom Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77      1037\n",
      "           1       0.47      0.86      0.61       365\n",
      "\n",
      "    accuracy                           0.71      1402\n",
      "   macro avg       0.70      0.76      0.69      1402\n",
      "weighted avg       0.81      0.71      0.73      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define individual models with tuned hyperparameters if available\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "log_reg_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a VotingClassifier ensemble using soft voting and custom weights\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('rf', rf_model), ('lr', log_reg_model), ('gb', gb_model)],\n",
    "    voting='soft',  # 'soft' voting averages predicted probabilities\n",
    "    weights=[2, 1, 2]  # Adjust these weights as needed based on performance\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on the SMOTE-resampled training data\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the ensemble model on the validation set\n",
    "y_val_probs_ensemble = ensemble_model.predict_proba(X_val_preprocessed)[:, 1]\n",
    "custom_threshold = 0.3\n",
    "y_val_pred_ensemble = (y_val_probs_ensemble >= custom_threshold).astype(int)\n",
    "\n",
    "print(\"Ensemble Validation Accuracy:\", accuracy_score(y_val_encoded, y_val_pred_ensemble) * 100)\n",
    "print(\"Ensemble Classification Report with Custom Threshold:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b297beeb-6359-4b59-88db-c1f66d8a4c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE + undersampling: [1545 1545]\n",
      "Training and evaluating Random Forest with combined SMOTE and undersampling...\n",
      "Random Forest Validation Accuracy: 76.82%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84      1037\n",
      "           1       0.54      0.67      0.60       365\n",
      "\n",
      "    accuracy                           0.77      1402\n",
      "   macro avg       0.71      0.74      0.72      1402\n",
      "weighted avg       0.79      0.77      0.78      1402\n",
      "\n",
      "Training and evaluating SVM with combined SMOTE and undersampling...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression with combined SMOTE and undersampling...\n",
      "Logistic Regression Validation Accuracy: 74.32%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1037\n",
      "           1       0.50      0.76      0.61       365\n",
      "\n",
      "    accuracy                           0.74      1402\n",
      "   macro avg       0.70      0.75      0.71      1402\n",
      "weighted avg       0.79      0.74      0.76      1402\n",
      "\n",
      "Training and evaluating Decision Tree with combined SMOTE and undersampling...\n",
      "Decision Tree Validation Accuracy: 69.76%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78      1037\n",
      "           1       0.44      0.60      0.51       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.64      0.67      0.65      1402\n",
      "weighted avg       0.74      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting with combined SMOTE and undersampling...\n",
      "Gradient Boosting Validation Accuracy: 76.32%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.83      1037\n",
      "           1       0.53      0.76      0.63       365\n",
      "\n",
      "    accuracy                           0.76      1402\n",
      "   macro avg       0.72      0.76      0.73      1402\n",
      "weighted avg       0.80      0.76      0.77      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline combining SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy=0.5)),  # Apply SMOTE with a target ratio\n",
    "    ('undersample', RandomUnderSampler(random_state=42))  # Then apply undersampling\n",
    "])\n",
    "\n",
    "# Apply the resampling pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "\n",
    "# Verify the new class distribution after combined resampling\n",
    "print(\"Class distribution after SMOTE + undersampling:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Train and evaluate models with this new balanced data\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name} with combined SMOTE and undersampling...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e4eb91-c863-4584-b63c-ea5287ac7d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m ada\u001b[38;5;241m.\u001b[39mfit_resample(X_train_preprocessed, y_train)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Verify class distribution after SMOTE\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass distribution after SMOTE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y_train_resampled))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Step 7: Define models to evaluate\u001b[39;00m\n\u001b[0;32m     60\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m'\u001b[39m: SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     66\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Yes'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "ada = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ada.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f83b2b-1ea0-4013-8cd6-34795764d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [3091 3109]\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 78.53%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      1037\n",
      "           1       0.60      0.55      0.57       365\n",
      "\n",
      "    accuracy                           0.79      1402\n",
      "   macro avg       0.72      0.71      0.71      1402\n",
      "weighted avg       0.78      0.79      0.78      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 72.40%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.79      1037\n",
      "           1       0.48      0.82      0.61       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.70      0.75      0.70      1402\n",
      "weighted avg       0.80      0.72      0.74      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 71.47%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80      1037\n",
      "           1       0.46      0.58      0.51       365\n",
      "\n",
      "    accuracy                           0.71      1402\n",
      "   macro avg       0.65      0.67      0.66      1402\n",
      "weighted avg       0.74      0.71      0.72      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 77.67%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84      1037\n",
      "           1       0.56      0.71      0.62       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.72      0.75      0.73      1402\n",
      "weighted avg       0.80      0.78      0.78      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7853067047075606, 'SVM': 0.6961483594864479, 'Logistic Regression': 0.723965763195435, 'Decision Tree': 0.7146932952924394, 'Gradient Boosting': 0.7767475035663338}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.017161       0.013638   \n",
      "num__tenure                                       0.190456       0.159873   \n",
      "num__monthly_charges                              0.157685       0.187250   \n",
      "cat__gender_Male                                  0.037910       0.035844   \n",
      "cat__partner_Yes                                  0.032696       0.025140   \n",
      "cat__dependents_Yes                               0.030265       0.021672   \n",
      "cat__phone_service_Yes                            0.005804       0.001649   \n",
      "cat__multiple_lines_No phone service              0.004890       0.007494   \n",
      "cat__multiple_lines_Yes                           0.028194       0.020788   \n",
      "cat__internet_service_Fiber optic                 0.039363       0.033955   \n",
      "cat__internet_service_No                          0.007387       0.000000   \n",
      "cat__online_security_No internet service          0.003802       0.000647   \n",
      "cat__online_security_Yes                          0.042451       0.023135   \n",
      "cat__online_backup_No internet service            0.005774       0.004018   \n",
      "cat__online_backup_Yes                            0.026986       0.022341   \n",
      "cat__device_protection_No internet service        0.007004       0.000000   \n",
      "cat__device_protection_Yes                        0.026294       0.014712   \n",
      "cat__tech_support_No internet service             0.001591       0.000000   \n",
      "cat__tech_support_Yes                             0.039665       0.021577   \n",
      "cat__streaming_t_v_No internet service            0.005659       0.000647   \n",
      "cat__streaming_t_v_Yes                            0.021214       0.015474   \n",
      "cat__streaming_movies_No internet service         0.007699       0.000435   \n",
      "cat__streaming_movies_Yes                         0.022984       0.034637   \n",
      "cat__contract_One year                            0.041657       0.111621   \n",
      "cat__contract_Two year                            0.072001       0.160342   \n",
      "cat__paperless_billing_Yes                        0.043595       0.030734   \n",
      "cat__payment_method_Credit card (automatic)       0.021848       0.018158   \n",
      "cat__payment_method_Electronic check              0.041479       0.019307   \n",
      "cat__payment_method_Mailed check                  0.016488       0.014912   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.002061  \n",
      "num__tenure                                           0.115966  \n",
      "num__monthly_charges                                  0.022368  \n",
      "cat__gender_Male                                      0.033137  \n",
      "cat__partner_Yes                                      0.018498  \n",
      "cat__dependents_Yes                                   0.033914  \n",
      "cat__phone_service_Yes                                0.001848  \n",
      "cat__multiple_lines_No phone service                  0.001257  \n",
      "cat__multiple_lines_Yes                               0.038052  \n",
      "cat__internet_service_Fiber optic                     0.065224  \n",
      "cat__internet_service_No                              0.000829  \n",
      "cat__online_security_No internet service              0.000000  \n",
      "cat__online_security_Yes                              0.044134  \n",
      "cat__online_backup_No internet service                0.003718  \n",
      "cat__online_backup_Yes                                0.012768  \n",
      "cat__device_protection_No internet service            0.000621  \n",
      "cat__device_protection_Yes                            0.009310  \n",
      "cat__tech_support_No internet service                 0.000000  \n",
      "cat__tech_support_Yes                                 0.026361  \n",
      "cat__streaming_t_v_No internet service                0.002821  \n",
      "cat__streaming_t_v_Yes                                0.017245  \n",
      "cat__streaming_movies_No internet service             0.003724  \n",
      "cat__streaming_movies_Yes                             0.036710  \n",
      "cat__contract_One year                                0.129728  \n",
      "cat__contract_Two year                                0.231260  \n",
      "cat__paperless_billing_Yes                            0.068236  \n",
      "cat__payment_method_Credit card (automatic)           0.017614  \n",
      "cat__payment_method_Electronic check                  0.056102  \n",
      "cat__payment_method_Mailed check                      0.006493  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "ada = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ada.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64258045-7690-4d5f-9f67-9046936e04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyEnsemble Validation Accuracy: 75.320970042796\n",
      "EasyEnsemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.91      0.74      0.82      1037\n",
      "         Yes       0.52      0.80      0.63       365\n",
      "\n",
      "    accuracy                           0.75      1402\n",
      "   macro avg       0.72      0.77      0.72      1402\n",
      "weighted avg       0.81      0.75      0.77      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define and fit the EasyEnsembleClassifier\n",
    "model = EasyEnsembleClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = model.predict(X_val_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"EasyEnsemble Validation Accuracy:\", accuracy_score(y_val, y_val_pred) * 100)\n",
    "print(\"EasyEnsemble Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "344af228-bb96-4a7c-9e30-f462cf655fdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 360 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n240 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1201, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1263, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 521, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Female'\n\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n    raise ValueError(\nValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1182, in fit\n    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\nValueError: l1_ratio must be specified when penalty is elasticnet.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run grid search\u001b[39;00m\n\u001b[0;32m     13\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(model, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Best model parameters\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    945\u001b[0m     )\n\u001b[1;32m--> 947\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 360 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n240 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1201, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1263, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 521, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Female'\n\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1172, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 75, in _check_solver\n    raise ValueError(\nValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n\n--------------------------------------------------------------------------------\n60 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1182, in fit\n    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\nValueError: l1_ratio must be specified when penalty is elasticnet.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 500]\n",
    "}\n",
    "\n",
    "# Run grid search\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ae8879a-5e48-475a-b1fa-f0b08dd933dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 81 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n81 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1263, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 521, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Female'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Hyperparameter tuning for Random Forest\u001b[39;00m\n\u001b[0;32m     17\u001b[0m grid_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m), param_grid_rf, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m grid_rf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m best_rf \u001b[38;5;241m=\u001b[39m grid_rf\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Hyperparameter tuning for SVC\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:947\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    945\u001b[0m     )\n\u001b[1;32m--> 947\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    949\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:536\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    530\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    531\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    540\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 81 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n81 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 363, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 650, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1263, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 997, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 521, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\asens\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'Female'\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "param_grid_svc = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Hyperparameter tuning for Random Forest\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3, scoring='accuracy')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for SVC\n",
    "grid_svc = GridSearchCV(SVC(probability=True, random_state=42), param_grid_svc, cv=3, scoring='accuracy')\n",
    "grid_svc.fit(X_train, y_train)\n",
    "best_svc = grid_svc.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Gradient Boosting\n",
    "grid_gb = GridSearchCV(GradientBoostingClassifier(random_state=42), param_grid_gb, cv=3, scoring='accuracy')\n",
    "grid_gb.fit(X_train, y_train)\n",
    "best_gb = grid_gb.best_estimator_\n",
    "\n",
    "# Step 2: Integrate tuned models into a stacking classifier\n",
    "base_estimators = [\n",
    "    ('rf', best_rf),\n",
    "    ('svc', best_svc),\n",
    "    ('gb', best_gb)\n",
    "]\n",
    "\n",
    "# Meta model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Define the stacking classifier\n",
    "stacking_clf = StackingClassifier(estimators=base_estimators, final_estimator=meta_model)\n",
    "\n",
    "# Fit the stacking classifier on training data\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred = stacking_clf.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3df2ffca-df7e-44f9-95f7-0568d0b8a059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__C': 1, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n",
      "Best Cross-Validation Accuracy: 0.7936259571090858\n",
      "\n",
      "Classification Report on Validation Set:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.83      0.94      0.88      1037\n",
      "         Yes       0.72      0.45      0.56       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.77      0.70      0.72      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `data_no_total` is your DataFrame and 'churn' is the target column\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define the ColumnTransformer for preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Predict on validation set with the best model\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "# Classification report for validation set\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\nClassification Report on Validation Set:\\n\", classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78168473-4977-4e1b-a41e-5a1e6f89f7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
