{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b379e-22cf-4586-a210-c63cebd0ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_comparison_table(experiment_name=\"Telecom_customer_churn\", tag_filters=None):\n",
    "    \"\"\"\n",
    "    Generate a comparison table for model performance metrics within a specified MLflow experiment,\n",
    "    with an option to filter by tags.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): The name of the MLflow experiment to retrieve the model runs from. \n",
    "      Default is \"Telecom_customer_churn\".\n",
    "    - tag_filters (dict): A dictionary of tags to filter runs by, e.g., {\"feature_selection\": \"SelectKBest\"}.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the model name and selected metrics for each run \n",
    "      in the specified experiment.\n",
    "    \"\"\"\n",
    "    # Get the experiment ID\n",
    "    experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "    # Retrieve all runs in the experiment\n",
    "    runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "    # Filter runs based on tag filters, if specified\n",
    "    if tag_filters:\n",
    "        for tag_key, tag_value in tag_filters.items():\n",
    "            runs = runs[runs[f\"tags.{tag_key}\"] == tag_value]\n",
    "\n",
    "    # Define the metrics you want to include in the comparison\n",
    "    metrics_to_include = [\n",
    "        \"accuracy\", \"accuracy_cv\", \"precision_yes\",\"recall_yes\",\"precision_no\", \"recall_no\", \n",
    "        \"roc_auc_val\", \"roc_auc_cv\", \"f1_score_no\", \"f1_score_yes\", \n",
    "        \"weighted_avg_precision\", \"weighted_avg_recall\", \"weighted_avg_f1_score\"\n",
    "    ]\n",
    "    \n",
    "    # Create an empty list to store each run's data\n",
    "    model_data = []\n",
    "\n",
    "    # Loop through each run to retrieve metrics and parameters\n",
    "    for _, run in runs.iterrows():\n",
    "        # Get the model name from parameters or the run name\n",
    "        model_name = run.get(\"tags.mlflow.runName\", \"Unknown Model\")\n",
    "        \n",
    "        # Extract the metrics\n",
    "        metrics = {metric: run.get(f\"metrics.{metric}\", None) for metric in metrics_to_include}\n",
    "        \n",
    "        # Add model name to the metrics dictionary\n",
    "        metrics[\"model_name\"] = model_name\n",
    "        \n",
    "        # Append to the list\n",
    "        model_data.append(metrics)\n",
    "\n",
    "    # Create a DataFrame from the list\n",
    "    comparison_df = pd.DataFrame(model_data)\n",
    "\n",
    "    # Order columns to show model name first\n",
    "    comparison_df = comparison_df[[\"model_name\"] + metrics_to_include]\n",
    "\n",
    "    return comparison_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eaa05a-4d41-42e6-8932-ab18097f7fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline_results(pipeline, X_train, y_train, X_val, y_val, run_name=\"Gradient_Boosting_Model_K_Best_features\", tags={\"feature_selection\": \"SelectKBest\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ee3ee0-6cac-4574-81ba-4f581344287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_pipeline_results(pipeline, X_train, y_train, X_val, y_val, run_name, cv_folds=5, tags=None):\n",
    "    \"\"\"\n",
    "    Log the current pipeline configuration and results to MLflow with optional tags.\n",
    "    \n",
    "    Parameters:\n",
    "    - pipeline: The sklearn pipeline, already configured with preprocessing and classifier.\n",
    "    - X_train, y_train: Training data and labels.\n",
    "    - X_val, y_val: Validation data and labels.\n",
    "    - run_name: Name of the MLflow run for easy identification.\n",
    "    - cv_folds: Number of folds for cross-validation.\n",
    "    - tags (dict): Optional dictionary of tags to attach to the run, e.g., {\"feature_selection\": \"SelectKBest\"}.\n",
    "    \"\"\"\n",
    "    # Set a default tag if none is provided\n",
    "    if tags is None:\n",
    "        tags = {\"feature_selection\": \"no_changes\"}  # default tag for baseline runs\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Set tags for this run\n",
    "        for tag_key, tag_value in tags.items():\n",
    "            mlflow.set_tag(tag_key, tag_value)\n",
    "        \n",
    "        # (The rest of your function remains the same...)\n",
    "        # Example code for logging metrics and parameters here...\n",
    "\n",
    "        # Cross-validation setup\n",
    "        cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
    "        accuracy_cv = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy').mean()\n",
    "        \n",
    "        # Cross-validated predictions for AUC calculations\n",
    "        y_train_pred_proba = cross_val_predict(pipeline, X_train, y_train, cv=cv, method=\"predict_proba\")\n",
    "        \n",
    "        # Calculate cross-validated ROC-AUC for the positive class (e.g., \"Yes\" class)\n",
    "        roc_auc_cv = roc_auc_score(y_train, y_train_pred_proba[:, 1])\n",
    "        mlflow.log_metric(\"roc_auc_cv\", roc_auc_cv)\n",
    "        \n",
    "        # Calculate cross-validated Precision-Recall AUC for the positive class\n",
    "        precision, recall, _ = precision_recall_curve(y_train, y_train_pred_proba[:, 1], pos_label=\"Yes\")\n",
    "        pr_auc_cv = auc(recall, precision)\n",
    "        mlflow.log_metric(\"pr_auc_cv\", pr_auc_cv)\n",
    "        \n",
    "        # Fit the pipeline on the training set\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_val_pred = pipeline.predict(X_val)\n",
    "        y_val_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        accuracy = accuracy_score(y_val, y_val_pred)\n",
    "        roc_auc_val = roc_auc_score(y_val, y_val_pred_proba)\n",
    "        \n",
    "        # Generate classification report for detailed metrics\n",
    "        report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "        \n",
    "        # Log overall accuracy and ROC-AUC on validation set\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"roc_auc_val\", roc_auc_val)\n",
    "        mlflow.log_metric(\"accuracy_cv\", accuracy_cv)\n",
    "        \n",
    "        # Log detailed class-specific metrics (for 'No' and 'Yes' labels)\n",
    "        for label in [\"No\", \"Yes\"]:\n",
    "            mlflow.log_metric(f\"precision_{label.lower()}\", report[label]['precision'])\n",
    "            mlflow.log_metric(f\"recall_{label.lower()}\", report[label]['recall'])\n",
    "            mlflow.log_metric(f\"f1_score_{label.lower()}\", report[label]['f1-score'])\n",
    "        \n",
    "        # Log macro and weighted averages\n",
    "        for avg_type in [\"macro avg\", \"weighted avg\"]:\n",
    "            mlflow.log_metric(f\"{avg_type.replace(' ', '_')}_precision\", report[avg_type]['precision'])\n",
    "            mlflow.log_metric(f\"{avg_type.replace(' ', '_')}_recall\", report[avg_type]['recall'])\n",
    "            mlflow.log_metric(f\"{avg_type.replace(' ', '_')}_f1_score\", report[avg_type]['f1-score'])\n",
    "\n",
    "        # Log model name and parameters\n",
    "        classifier = pipeline.named_steps['classifier']\n",
    "        mlflow.log_param(\"model\", classifier.__class__.__name__)\n",
    "        for param, value in classifier.get_params().items():\n",
    "            mlflow.log_param(param, value)\n",
    "        \n",
    "        # Log feature importances if available\n",
    "       # if hasattr(classifier, 'feature_importances_'):\n",
    "            #feature_names = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "            #feature_importances = classifier.feature_importances_\n",
    "            #importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "            #mlflow.log_text(importance_df.to_string(), \"feature_importances.txt\")\n",
    "        \n",
    "        # Log the entire pipeline as a model\n",
    "        mlflow.sklearn.log_model(pipeline, \"pipeline_model\")\n",
    "        \n",
    "        print(f\"Logged {run_name} with validation accuracy: {accuracy * 100:.2f}%, ROC-AUC (CV): {roc_auc_cv:.4f}, and PR-AUC (CV): {pr_auc_cv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f77c516-78a8-45ce-8746-56d4e734cf83",
   "metadata": {},
   "source": [
    "1. As it can be seen all the models demonstrate relatively high accuracy, indicating they perform well overall. However, they struggle to accurately classify the \"Yes\" class, as shown by the lower precision, recall, and F1 scores for \"Yes\" across the board. This suggests that, while the models are good at identifying \"No\" cases, they have difficulty correctly identifying \"Yes\" classes. Which in our case is not ideal, as the aim is to predict customer churn.\n",
    "2. Summary of models:\n",
    "The Logistic Regression Model is the great overall, with high accuracy and balanced metrics, but the SVC is a strong alternative for slightly better churn detection, though with more false positives; Random Forest and Gradient Boosting show decent performance but miss more churn cases, and Decision Tree is the least effective due to lower accuracy and poor recall for churn cases.\n",
    "3. Feature importances: Random Forest, Gradient Boosting and Decission Tree have the attribute feature_importances that helps us understand which input features (variables) are most influential in making predictions for a model. A text file with the feature importances can be found inside one of the three models pages in Mlflow. Example:(Click on Gradient_Boosting_Model and then navigate to the Artifacts section. File name feature_importances.txt). But here I will summarize the findings and compare them with my previous Chi squared tests and Cramer's V to see whether they are the same\n",
    "4. The most important features for predicting churn across models include tenure, monthly charges, fiber optic internet service, contract duration, and electronic check payment method, while moderately important features are online security, tech support, streaming services, paperless billing, and having dependents or a partner.\n",
    "5. There is substantial overlap between the features identified as important by Chi-Square, Cramér's V, and model feature importance scores. Key features like contract, internet_service_Fiber_optic, and num_tenure consistently appear as strong predictors of churn, making them robust variables to focus on in your analysis and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80709151-38e5-4302-b5e3-9945ea813154",
   "metadata": {},
   "source": [
    "### Models with only the best features:(SelectKbest)\n",
    "1. Now I will run the same models using only the best features. Now we saw which features are important, but instead of explicitly selecting those features I will just add another step in the pipeline, which I will call feature_select, and I will do that with the function (Select K Best).\n",
    "2. SelectKBest is a feature selection method in machine learning that helps reduce the number of input features by selecting only the most important ones based on a scoring function. It’s particularly useful for simplifying models, improving performance, and reducing overfitting, especially when dealing with high-dimensional data.\n",
    "3. In the pipeline I will simply add another line of code under the preprocessor tuple: ('feature_selection', SelectKBest(k = 10), this will select only the 10 most significant features to use to run the model.\n",
    "4. And when I log the run I will add one more addition. tags= \"feature_selection\": \"SelectKBest\"\n",
    "5. Also I will comment the section of my function that gets the feature importances as we already know that and using SelectKbest makes it redundant.\n",
    "6. I have decided to now proceed with testing only the logistic regression, SVC and Gradient boosting as they showed best performance and I want to see how these alterations influence their performance before I get to the best one.\n",
    "7. Again I will provide a table of the models metrics. I will try the models with only the most important features and then add the rest moderately important. SelectKbest = 6 and then 10. This is more than half the features, so let's see what is going to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a0cb8e-dcd3-41cf-937a-aef06b7be118",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline_results(pipeline, X_train, y_train, X_val, y_val, run_name=\"SVC_Model_Balanced_class_weight\", tags={\"class_weight\": \"class_weight\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbfb6a-1b03-4c2d-99b2-b13a26d53c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipeline_results(pipeline, X_train, y_train, X_val, y_val, run_name=\"SVC_Model_Balanced_class_weight\", tags={\"feature_selection\": \"SelectKBest\"}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
