{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21623389-664a-4826-ada9-14947c358d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f87ae0b-9569-453e-8dac-d5efb63ebf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_data_telecom.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1b72376-ccf2-4c41-b6a6-5b4c54563a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_total = data.drop(['total_charges'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab0c0d9-e6e9-41ff-ae4c-c69bacbfddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4206, 29)\n",
      "Validation set shape: (1402, 29)\n",
      "Test set shape: (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume `data` is the DataFrame and 'contract' is the feature where we want 'Month-to-month' to remain\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Split the data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Separate out 'contract' and other categorical/numerical features\n",
    "contract_feature = ['contract']\n",
    "other_categorical_features = [col for col in X_train.select_dtypes(include=['object']).columns if col != 'contract']\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define separate transformers for contract and other categorical features\n",
    "contract_transformer = OneHotEncoder(drop=['Two year'], sparse_output=False)  # Dropping 'Two year' to keep 'Month-to-month'\n",
    "other_categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)  # Default drop first for other categories\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('contract', contract_transformer, contract_feature),\n",
    "        ('cat', other_categorical_transformer, other_categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the datasets\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Output the shapes of the processed datasets\n",
    "print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape:\", X_test_preprocessed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "572a11a1-40d2-4868-a1b1-027dba3c26fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (4206, 29)\n",
      "Validation set shape: (1402, 29)\n",
      "Test set shape: (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Define a preprocessor that will be fit only on the training data\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data only\n",
    "train_preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training, validation, and test sets using the same preprocessor\n",
    "X_train_preprocessed = train_preprocessor.transform(X_train)\n",
    "X_val_preprocessed = train_preprocessor.transform(X_val)\n",
    "X_test_preprocessed = train_preprocessor.transform(X_test)\n",
    "\n",
    "# Output the shapes of the processed datasets to confirm transformation\n",
    "print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape:\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598bce64-ad30-4c6e-b837-df7ad1aa0c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape after SMOTE: (6182, 29)\n",
      "Validation set shape (untouched): (1402, 29)\n",
      "Test set shape (untouched): (1402, 29)\n"
     ]
    }
   ],
   "source": [
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Step 1: Split data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define a preprocessor for scaling and encoding\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data only\n",
    "train_preprocessor.fit(X_train)\n",
    "\n",
    "# Transform the training set, then apply SMOTE\n",
    "X_train_preprocessed = train_preprocessor.transform(X_train)\n",
    "X_val_preprocessed = train_preprocessor.transform(X_val)\n",
    "X_test_preprocessed = train_preprocessor.transform(X_test)\n",
    "\n",
    "# Step 4: Apply SMOTE only on the preprocessed training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "# Output the shapes to confirm transformations\n",
    "print(\"Training set shape after SMOTE:\", X_train_resampled.shape)\n",
    "print(\"Validation set shape (untouched):\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape (untouched):\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3271d72a-1b48-4910-9d16-7867823ac766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 79.96%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.83      0.92      0.87      1037\n",
      "         Yes       0.66      0.47      0.55       365\n",
      "\n",
      "    accuracy                           0.80      1402\n",
      "   macro avg       0.75      0.69      0.71      1402\n",
      "weighted avg       0.79      0.80      0.79      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 81.81%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.91      0.88      1037\n",
      "         Yes       0.69      0.55      0.61       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.77      0.73      0.75      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 82.03%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.93      0.88      1037\n",
      "         Yes       0.71      0.52      0.60       365\n",
      "\n",
      "    accuracy                           0.82      1402\n",
      "   macro avg       0.78      0.72      0.74      1402\n",
      "weighted avg       0.81      0.82      0.81      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 75.18%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.83      0.84      0.83      1037\n",
      "         Yes       0.52      0.49      0.51       365\n",
      "\n",
      "    accuracy                           0.75      1402\n",
      "   macro avg       0.68      0.67      0.67      1402\n",
      "weighted avg       0.75      0.75      0.75      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 81.17%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.92      0.88      1037\n",
      "         Yes       0.68      0.52      0.59       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.72      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7995720399429387, 'SVM': 0.81811697574893, 'Logistic Regression': 0.8202567760342369, 'Decision Tree': 0.7517831669044223, 'Gradient Boosting': 0.81169757489301}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.023060       0.015939   \n",
      "num__tenure                                       0.230052       0.200448   \n",
      "num__monthly_charges                              0.223593       0.288999   \n",
      "contract__contract_Month-to-month                 0.072347       0.162640   \n",
      "contract__contract_One year                       0.013677       0.003167   \n",
      "cat__gender_Male                                  0.035806       0.034260   \n",
      "cat__partner_Yes                                  0.027071       0.028338   \n",
      "cat__dependents_Yes                               0.023715       0.022133   \n",
      "cat__phone_service_Yes                            0.005818       0.000000   \n",
      "cat__multiple_lines_No phone service              0.005510       0.005149   \n",
      "cat__multiple_lines_Yes                           0.025164       0.020358   \n",
      "cat__internet_service_Fiber optic                 0.030864       0.047743   \n",
      "cat__internet_service_No                          0.005688       0.000000   \n",
      "cat__online_security_No internet service          0.005689       0.000000   \n",
      "cat__online_security_Yes                          0.030467       0.017324   \n",
      "cat__online_backup_No internet service            0.005021       0.000000   \n",
      "cat__online_backup_Yes                            0.026681       0.016956   \n",
      "cat__device_protection_No internet service        0.004719       0.000000   \n",
      "cat__device_protection_Yes                        0.023201       0.023518   \n",
      "cat__tech_support_No internet service             0.005009       0.000000   \n",
      "cat__tech_support_Yes                             0.027146       0.013518   \n",
      "cat__streaming_t_v_No internet service            0.004676       0.007639   \n",
      "cat__streaming_t_v_Yes                            0.021163       0.009337   \n",
      "cat__streaming_movies_No internet service         0.006175       0.000000   \n",
      "cat__streaming_movies_Yes                         0.020432       0.010075   \n",
      "cat__paperless_billing_Yes                        0.030496       0.022863   \n",
      "cat__payment_method_Credit card (automatic)       0.016956       0.016127   \n",
      "cat__payment_method_Electronic check              0.033826       0.017438   \n",
      "cat__payment_method_Mailed check                  0.015978       0.016032   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.006135  \n",
      "num__tenure                                           0.196267  \n",
      "num__monthly_charges                                  0.102383  \n",
      "contract__contract_Month-to-month                     0.412451  \n",
      "contract__contract_One year                           0.013459  \n",
      "cat__gender_Male                                      0.002462  \n",
      "cat__partner_Yes                                      0.000733  \n",
      "cat__dependents_Yes                                   0.006135  \n",
      "cat__phone_service_Yes                                0.003751  \n",
      "cat__multiple_lines_No phone service                  0.008716  \n",
      "cat__multiple_lines_Yes                               0.010013  \n",
      "cat__internet_service_Fiber optic                     0.093454  \n",
      "cat__internet_service_No                              0.004689  \n",
      "cat__online_security_No internet service              0.001403  \n",
      "cat__online_security_Yes                              0.023294  \n",
      "cat__online_backup_No internet service                0.004714  \n",
      "cat__online_backup_Yes                                0.002004  \n",
      "cat__device_protection_No internet service            0.007225  \n",
      "cat__device_protection_Yes                            0.000000  \n",
      "cat__tech_support_No internet service                 0.007650  \n",
      "cat__tech_support_Yes                                 0.009121  \n",
      "cat__streaming_t_v_No internet service                0.001641  \n",
      "cat__streaming_t_v_Yes                                0.006207  \n",
      "cat__streaming_movies_No internet service             0.011125  \n",
      "cat__streaming_movies_Yes                             0.009710  \n",
      "cat__paperless_billing_Yes                            0.022748  \n",
      "cat__payment_method_Credit card (automatic)           0.003822  \n",
      "cat__payment_method_Electronic check                  0.027952  \n",
      "cat__payment_method_Mailed check                      0.000738  \n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    model.fit(X_train_preprocessed, y_train)\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "284bbed7-efe0-4309-a420-c61e5fe9f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected features, based on their transformed names\n",
    "selected_feature_names = [\n",
    "    'num__tenure', \n",
    "    'num__monthly_charges', \n",
    "    'cat__internet_service_Fiber optic', \n",
    "    'contract__contract_Month-to-month', \n",
    "    'cat__contract_One year', \n",
    "    'cat__paperless_billing_Yes', \n",
    "    'cat__payment_method_Electronic check', \n",
    "    'cat__online_security_Yes', \n",
    "    'cat__dependents_Yes', \n",
    "    'cat__payment_method_Credit card (automatic)',\n",
    "    'cat__gender_Male',\n",
    "    'cat__partner_Yes',\n",
    "    'cat__tech_support_Yes',\n",
    "    'cat__online_backup_Yes',\n",
    "    'cat__streaming_movies_Yes'\n",
    "    \n",
    "]\n",
    "\n",
    "# Get all transformed feature names from the preprocessor\n",
    "all_feature_names = train_preprocessor.get_feature_names_out()\n",
    "\n",
    "# Find the indices of selected features in the preprocessed dataset\n",
    "selected_feature_indices = [i for i, feature in enumerate(all_feature_names) if feature in selected_feature_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f6a2a3-20a4-4167-ada7-7fc23d98b5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Training set shape: (4206, 14)\n",
      "Filtered Validation set shape: (1402, 14)\n",
      "Filtered Test set shape: (1402, 14)\n"
     ]
    }
   ],
   "source": [
    "# Filter each preprocessed dataset to keep only selected features\n",
    "X_train_selected = X_train_preprocessed[:, selected_feature_indices]\n",
    "X_val_selected = X_val_preprocessed[:, selected_feature_indices]\n",
    "X_test_selected = X_test_preprocessed[:, selected_feature_indices]\n",
    "\n",
    "# Check shapes to confirm\n",
    "print(\"Filtered Training set shape:\", X_train_selected.shape)\n",
    "print(\"Filtered Validation set shape:\", X_val_selected.shape)\n",
    "print(\"Filtered Test set shape:\", X_test_selected.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886601e6-0a02-42cc-8e7a-8d87da8a696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating Random Forest with selected features...\n",
      "Random Forest Validation Accuracy with selected features: 77.60%\n",
      "\n",
      "Random Forest Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.89      0.86      1037\n",
      "         Yes       0.59      0.44      0.50       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.71      0.67      0.68      1402\n",
      "weighted avg       0.76      0.78      0.76      1402\n",
      "\n",
      "Training and evaluating SVM with selected features...\n",
      "SVM Validation Accuracy with selected features: 80.74%\n",
      "\n",
      "SVM Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.91      0.88      1037\n",
      "         Yes       0.67      0.51      0.58       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.71      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "Training and evaluating Logistic Regression with selected features...\n",
      "Logistic Regression Validation Accuracy with selected features: 81.03%\n",
      "\n",
      "Logistic Regression Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.92      0.88      1037\n",
      "         Yes       0.69      0.50      0.58       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.71      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "Training and evaluating Decision Tree with selected features...\n",
      "Decision Tree Validation Accuracy with selected features: 71.54%\n",
      "\n",
      "Decision Tree Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.81      0.81      0.81      1037\n",
      "         Yes       0.45      0.45      0.45       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.63      0.63      0.63      1402\n",
      "weighted avg       0.72      0.72      0.72      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting with selected features...\n",
      "Gradient Boosting Validation Accuracy with selected features: 81.24%\n",
      "\n",
      "Gradient Boosting Classification Report with selected features:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.84      0.92      0.88      1037\n",
      "         Yes       0.68      0.52      0.59       365\n",
      "\n",
      "    accuracy                           0.81      1402\n",
      "   macro avg       0.76      0.72      0.73      1402\n",
      "weighted avg       0.80      0.81      0.80      1402\n",
      "\n",
      "\n",
      "Model Performance Summary with Selected Features:\n",
      "{'Random Forest': 0.776034236804565, 'SVM': 0.8074179743223966, 'Logistic Regression': 0.8102710413694721, 'Decision Tree': 0.7154065620542083, 'Gradient Boosting': 0.8124108416547788}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store model performance\n",
    "model_performance_selected = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name} with selected features...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_val_pred = model.predict(X_val_selected)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    model_performance_selected[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy with selected features: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report with selected features:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nModel Performance Summary with Selected Features:\")\n",
    "print(model_performance_selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4105fd3e-bbde-49b7-9590-e228d8ede6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [3091 3091]\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 78.60%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      1037\n",
      "           1       0.59      0.56      0.58       365\n",
      "\n",
      "    accuracy                           0.79      1402\n",
      "   macro avg       0.72      0.71      0.72      1402\n",
      "weighted avg       0.78      0.79      0.78      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 74.61%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1037\n",
      "           1       0.51      0.75      0.61       365\n",
      "\n",
      "    accuracy                           0.75      1402\n",
      "   macro avg       0.70      0.75      0.71      1402\n",
      "weighted avg       0.79      0.75      0.76      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 72.47%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.78      0.81      1037\n",
      "           1       0.48      0.55      0.51       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.65      0.67      0.66      1402\n",
      "weighted avg       0.74      0.72      0.73      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 78.03%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84      1037\n",
      "           1       0.56      0.72      0.63       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.73      0.76      0.74      1402\n",
      "weighted avg       0.80      0.78      0.79      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7860199714693296, 'SVM': 0.6961483594864479, 'Logistic Regression': 0.746077032810271, 'Decision Tree': 0.724679029957204, 'Gradient Boosting': 0.7803138373751783}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.017579       0.015295   \n",
      "num__tenure                                       0.197041       0.164662   \n",
      "num__monthly_charges                              0.156413       0.190125   \n",
      "cat__gender_Male                                  0.032803       0.039956   \n",
      "cat__partner_Yes                                  0.030093       0.021467   \n",
      "cat__dependents_Yes                               0.029341       0.024592   \n",
      "cat__phone_service_Yes                            0.005098       0.010648   \n",
      "cat__multiple_lines_No phone service              0.005417       0.000595   \n",
      "cat__multiple_lines_Yes                           0.024251       0.012532   \n",
      "cat__internet_service_Fiber optic                 0.039615       0.034586   \n",
      "cat__internet_service_No                          0.011168       0.000000   \n",
      "cat__online_security_No internet service          0.005086       0.000000   \n",
      "cat__online_security_Yes                          0.044685       0.025026   \n",
      "cat__online_backup_No internet service            0.006306       0.008762   \n",
      "cat__online_backup_Yes                            0.028862       0.020298   \n",
      "cat__device_protection_No internet service        0.008231       0.000000   \n",
      "cat__device_protection_Yes                        0.022927       0.015997   \n",
      "cat__tech_support_No internet service             0.003943       0.000000   \n",
      "cat__tech_support_Yes                             0.035304       0.017560   \n",
      "cat__streaming_t_v_No internet service            0.007342       0.000000   \n",
      "cat__streaming_t_v_Yes                            0.018876       0.011706   \n",
      "cat__streaming_movies_No internet service         0.008734       0.000000   \n",
      "cat__streaming_movies_Yes                         0.020469       0.021235   \n",
      "cat__contract_One year                            0.043453       0.136082   \n",
      "cat__contract_Two year                            0.069513       0.162832   \n",
      "cat__paperless_billing_Yes                        0.040523       0.024571   \n",
      "cat__payment_method_Credit card (automatic)       0.018838       0.009357   \n",
      "cat__payment_method_Electronic check              0.052554       0.019930   \n",
      "cat__payment_method_Mailed check                  0.015534       0.012187   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.001933  \n",
      "num__tenure                                           0.198356  \n",
      "num__monthly_charges                                  0.025857  \n",
      "cat__gender_Male                                      0.013192  \n",
      "cat__partner_Yes                                      0.009759  \n",
      "cat__dependents_Yes                                   0.039352  \n",
      "cat__phone_service_Yes                                0.004921  \n",
      "cat__multiple_lines_No phone service                  0.003072  \n",
      "cat__multiple_lines_Yes                               0.027533  \n",
      "cat__internet_service_Fiber optic                     0.084694  \n",
      "cat__internet_service_No                              0.013317  \n",
      "cat__online_security_No internet service              0.011206  \n",
      "cat__online_security_Yes                              0.049323  \n",
      "cat__online_backup_No internet service                0.006806  \n",
      "cat__online_backup_Yes                                0.017771  \n",
      "cat__device_protection_No internet service            0.002864  \n",
      "cat__device_protection_Yes                            0.007188  \n",
      "cat__tech_support_No internet service                 0.000000  \n",
      "cat__tech_support_Yes                                 0.016499  \n",
      "cat__streaming_t_v_No internet service                0.000297  \n",
      "cat__streaming_t_v_Yes                                0.013831  \n",
      "cat__streaming_movies_No internet service             0.005347  \n",
      "cat__streaming_movies_Yes                             0.018112  \n",
      "cat__contract_One year                                0.130855  \n",
      "cat__contract_Two year                                0.167858  \n",
      "cat__paperless_billing_Yes                            0.051184  \n",
      "cat__payment_method_Credit card (automatic)           0.011216  \n",
      "cat__payment_method_Electronic check                  0.061826  \n",
      "cat__payment_method_Mailed check                      0.005831  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda64be2-3924-4a09-a70f-ad2bc5f88568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with threshold 0.3: 71.61%\n",
      "Classification Report with Custom Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79      1037\n",
      "           1       0.47      0.75      0.58       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.68      0.73      0.68      1402\n",
      "weighted avg       0.78      0.72      0.73      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_curve, classification_report, accuracy_score\n",
    "\n",
    "# Fit the RandomForest model on resampled training data\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get predicted probabilities on the validation set\n",
    "y_val_probs = rf_model.predict_proba(X_val_preprocessed)[:, 1]  # Probabilities for class 1\n",
    "\n",
    "# Determine optimal threshold using the precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_val_encoded, y_val_probs)\n",
    "# Set a custom threshold, e.g., 0.3, to increase recall\n",
    "custom_threshold = 0.3\n",
    "y_val_pred_custom = (y_val_probs >= custom_threshold).astype(int)\n",
    "\n",
    "# Evaluate the model with the custom threshold\n",
    "print(f\"Validation Accuracy with threshold {custom_threshold}: {accuracy_score(y_val_encoded, y_val_pred_custom) * 100:.2f}%\")\n",
    "print(\"Classification Report with Custom Threshold:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e9be3d2-78f0-417f-b575-b415c97f35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Random Forest: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 10, 'class_weight': 'balanced_subsample'}\n",
      "Best Recall Score from Cross-Validation: 0.8622307182833772\n",
      "Validation Classification Report with Tuned Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.64      0.76      1037\n",
      "           1       0.46      0.86      0.60       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.69      0.75      0.68      1402\n",
      "weighted avg       0.81      0.70      0.72      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid for RandomForest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for Random Forest with cross-validation\n",
    "grid_search_rf = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=42),\n",
    "    param_distributions=param_grid_rf,\n",
    "    n_iter=20,\n",
    "    scoring='recall',  # Focusing on recall for the churn class\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the grid search on the resampled training data\n",
    "grid_search_rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Output the best parameters and best recall score\n",
    "print(\"Best Parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "print(\"Best Recall Score from Cross-Validation:\", grid_search_rf.best_score_)\n",
    "\n",
    "# Evaluate best model on validation set with custom threshold if needed\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_val_probs_rf = best_rf_model.predict_proba(X_val_preprocessed)[:, 1]\n",
    "y_val_pred_rf_custom = (y_val_probs_rf >= custom_threshold).astype(int)  # Using the same custom threshold from above\n",
    "\n",
    "print(\"Validation Classification Report with Tuned Random Forest:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_rf_custom))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc317f3-4b39-4e74-bf3f-8b3db50b1434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Validation Accuracy: 70.9700427960057\n",
      "Ensemble Classification Report with Custom Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.66      0.77      1037\n",
      "           1       0.47      0.86      0.61       365\n",
      "\n",
      "    accuracy                           0.71      1402\n",
      "   macro avg       0.70      0.76      0.69      1402\n",
      "weighted avg       0.81      0.71      0.73      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define individual models with tuned hyperparameters if available\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "log_reg_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a VotingClassifier ensemble using soft voting and custom weights\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('rf', rf_model), ('lr', log_reg_model), ('gb', gb_model)],\n",
    "    voting='soft',  # 'soft' voting averages predicted probabilities\n",
    "    weights=[2, 1, 2]  # Adjust these weights as needed based on performance\n",
    ")\n",
    "\n",
    "# Fit the ensemble model on the SMOTE-resampled training data\n",
    "ensemble_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate the ensemble model on the validation set\n",
    "y_val_probs_ensemble = ensemble_model.predict_proba(X_val_preprocessed)[:, 1]\n",
    "custom_threshold = 0.3\n",
    "y_val_pred_ensemble = (y_val_probs_ensemble >= custom_threshold).astype(int)\n",
    "\n",
    "print(\"Ensemble Validation Accuracy:\", accuracy_score(y_val_encoded, y_val_pred_ensemble) * 100)\n",
    "print(\"Ensemble Classification Report with Custom Threshold:\")\n",
    "print(classification_report(y_val_encoded, y_val_pred_ensemble))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b297beeb-6359-4b59-88db-c1f66d8a4c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE + undersampling: [1545 1545]\n",
      "Training and evaluating Random Forest with combined SMOTE and undersampling...\n",
      "Random Forest Validation Accuracy: 76.82%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84      1037\n",
      "           1       0.54      0.67      0.60       365\n",
      "\n",
      "    accuracy                           0.77      1402\n",
      "   macro avg       0.71      0.74      0.72      1402\n",
      "weighted avg       0.79      0.77      0.78      1402\n",
      "\n",
      "Training and evaluating SVM with combined SMOTE and undersampling...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression with combined SMOTE and undersampling...\n",
      "Logistic Regression Validation Accuracy: 74.32%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.74      0.81      1037\n",
      "           1       0.50      0.76      0.61       365\n",
      "\n",
      "    accuracy                           0.74      1402\n",
      "   macro avg       0.70      0.75      0.71      1402\n",
      "weighted avg       0.79      0.74      0.76      1402\n",
      "\n",
      "Training and evaluating Decision Tree with combined SMOTE and undersampling...\n",
      "Decision Tree Validation Accuracy: 69.76%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78      1037\n",
      "           1       0.44      0.60      0.51       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.64      0.67      0.65      1402\n",
      "weighted avg       0.74      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting with combined SMOTE and undersampling...\n",
      "Gradient Boosting Validation Accuracy: 76.32%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.76      0.83      1037\n",
      "           1       0.53      0.76      0.63       365\n",
      "\n",
      "    accuracy                           0.76      1402\n",
      "   macro avg       0.72      0.76      0.73      1402\n",
      "weighted avg       0.80      0.76      0.77      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "# Define a pipeline combining SMOTE and RandomUnderSampler\n",
    "resampling_pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42, sampling_strategy=0.5)),  # Apply SMOTE with a target ratio\n",
    "    ('undersample', RandomUnderSampler(random_state=42))  # Then apply undersampling\n",
    "])\n",
    "\n",
    "# Apply the resampling pipeline to the training data\n",
    "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "\n",
    "# Verify the new class distribution after combined resampling\n",
    "print(\"Class distribution after SMOTE + undersampling:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Train and evaluate models with this new balanced data\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name} with combined SMOTE and undersampling...\")\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45e4eb91-c863-4584-b63c-ea5287ac7d5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Yes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m X_train_resampled, y_train_resampled \u001b[38;5;241m=\u001b[39m ada\u001b[38;5;241m.\u001b[39mfit_resample(X_train_preprocessed, y_train)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Verify class distribution after SMOTE\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass distribution after SMOTE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mbincount(y_train_resampled))\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Step 7: Define models to evaluate\u001b[39;00m\n\u001b[0;32m     60\u001b[0m models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m'\u001b[39m: SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     66\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1031\u001b[0m, in \u001b[0;36mSeries.__array__\u001b[1;34m(self, dtype, copy)\u001b[0m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03mReturn the values as a NumPy array.\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;124;03m      dtype='datetime64[ns]')\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1031\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mand\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1033\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mview()\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Yes'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "ada = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ada.fit_resample(X_train_preprocessed, y_train)\n",
    "\n",
    "\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f83b2b-1ea0-4013-8cd6-34795764d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution after SMOTE: [3091 3109]\n",
      "Training and evaluating Random Forest...\n",
      "Random Forest Validation Accuracy: 78.53%\n",
      "\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      1037\n",
      "           1       0.60      0.55      0.57       365\n",
      "\n",
      "    accuracy                           0.79      1402\n",
      "   macro avg       0.72      0.71      0.71      1402\n",
      "weighted avg       0.78      0.79      0.78      1402\n",
      "\n",
      "Training and evaluating SVM...\n",
      "SVM Validation Accuracy: 69.61%\n",
      "\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.65      0.76      1037\n",
      "           1       0.45      0.82      0.58       365\n",
      "\n",
      "    accuracy                           0.70      1402\n",
      "   macro avg       0.68      0.74      0.67      1402\n",
      "weighted avg       0.79      0.70      0.71      1402\n",
      "\n",
      "Training and evaluating Logistic Regression...\n",
      "Logistic Regression Validation Accuracy: 72.40%\n",
      "\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.69      0.79      1037\n",
      "           1       0.48      0.82      0.61       365\n",
      "\n",
      "    accuracy                           0.72      1402\n",
      "   macro avg       0.70      0.75      0.70      1402\n",
      "weighted avg       0.80      0.72      0.74      1402\n",
      "\n",
      "Training and evaluating Decision Tree...\n",
      "Decision Tree Validation Accuracy: 71.47%\n",
      "\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.76      0.80      1037\n",
      "           1       0.46      0.58      0.51       365\n",
      "\n",
      "    accuracy                           0.71      1402\n",
      "   macro avg       0.65      0.67      0.66      1402\n",
      "weighted avg       0.74      0.71      0.72      1402\n",
      "\n",
      "Training and evaluating Gradient Boosting...\n",
      "Gradient Boosting Validation Accuracy: 77.67%\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84      1037\n",
      "           1       0.56      0.71      0.62       365\n",
      "\n",
      "    accuracy                           0.78      1402\n",
      "   macro avg       0.72      0.75      0.73      1402\n",
      "weighted avg       0.80      0.78      0.78      1402\n",
      "\n",
      "\n",
      "Model Performance Summary:\n",
      "{'Random Forest': 0.7853067047075606, 'SVM': 0.6961483594864479, 'Logistic Regression': 0.723965763195435, 'Decision Tree': 0.7146932952924394, 'Gradient Boosting': 0.7767475035663338}\n",
      "\n",
      "Feature Importances Summary:\n",
      "                                             Random Forest  Decision Tree  \\\n",
      "Feature                                                                     \n",
      "num__senior_citizen                               0.017161       0.013638   \n",
      "num__tenure                                       0.190456       0.159873   \n",
      "num__monthly_charges                              0.157685       0.187250   \n",
      "cat__gender_Male                                  0.037910       0.035844   \n",
      "cat__partner_Yes                                  0.032696       0.025140   \n",
      "cat__dependents_Yes                               0.030265       0.021672   \n",
      "cat__phone_service_Yes                            0.005804       0.001649   \n",
      "cat__multiple_lines_No phone service              0.004890       0.007494   \n",
      "cat__multiple_lines_Yes                           0.028194       0.020788   \n",
      "cat__internet_service_Fiber optic                 0.039363       0.033955   \n",
      "cat__internet_service_No                          0.007387       0.000000   \n",
      "cat__online_security_No internet service          0.003802       0.000647   \n",
      "cat__online_security_Yes                          0.042451       0.023135   \n",
      "cat__online_backup_No internet service            0.005774       0.004018   \n",
      "cat__online_backup_Yes                            0.026986       0.022341   \n",
      "cat__device_protection_No internet service        0.007004       0.000000   \n",
      "cat__device_protection_Yes                        0.026294       0.014712   \n",
      "cat__tech_support_No internet service             0.001591       0.000000   \n",
      "cat__tech_support_Yes                             0.039665       0.021577   \n",
      "cat__streaming_t_v_No internet service            0.005659       0.000647   \n",
      "cat__streaming_t_v_Yes                            0.021214       0.015474   \n",
      "cat__streaming_movies_No internet service         0.007699       0.000435   \n",
      "cat__streaming_movies_Yes                         0.022984       0.034637   \n",
      "cat__contract_One year                            0.041657       0.111621   \n",
      "cat__contract_Two year                            0.072001       0.160342   \n",
      "cat__paperless_billing_Yes                        0.043595       0.030734   \n",
      "cat__payment_method_Credit card (automatic)       0.021848       0.018158   \n",
      "cat__payment_method_Electronic check              0.041479       0.019307   \n",
      "cat__payment_method_Mailed check                  0.016488       0.014912   \n",
      "\n",
      "                                             Gradient Boosting  \n",
      "Feature                                                         \n",
      "num__senior_citizen                                   0.002061  \n",
      "num__tenure                                           0.115966  \n",
      "num__monthly_charges                                  0.022368  \n",
      "cat__gender_Male                                      0.033137  \n",
      "cat__partner_Yes                                      0.018498  \n",
      "cat__dependents_Yes                                   0.033914  \n",
      "cat__phone_service_Yes                                0.001848  \n",
      "cat__multiple_lines_No phone service                  0.001257  \n",
      "cat__multiple_lines_Yes                               0.038052  \n",
      "cat__internet_service_Fiber optic                     0.065224  \n",
      "cat__internet_service_No                              0.000829  \n",
      "cat__online_security_No internet service              0.000000  \n",
      "cat__online_security_Yes                              0.044134  \n",
      "cat__online_backup_No internet service                0.003718  \n",
      "cat__online_backup_Yes                                0.012768  \n",
      "cat__device_protection_No internet service            0.000621  \n",
      "cat__device_protection_Yes                            0.009310  \n",
      "cat__tech_support_No internet service                 0.000000  \n",
      "cat__tech_support_Yes                                 0.026361  \n",
      "cat__streaming_t_v_No internet service                0.002821  \n",
      "cat__streaming_t_v_Yes                                0.017245  \n",
      "cat__streaming_movies_No internet service             0.003724  \n",
      "cat__streaming_movies_Yes                             0.036710  \n",
      "cat__contract_One year                                0.129728  \n",
      "cat__contract_Two year                                0.231260  \n",
      "cat__paperless_billing_Yes                            0.068236  \n",
      "cat__payment_method_Credit card (automatic)           0.017614  \n",
      "cat__payment_method_Electronic check                  0.056102  \n",
      "cat__payment_method_Mailed check                      0.006493  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Step 1: Data Preparation - Split data\n",
    "features = data_no_total.columns.drop('churn')\n",
    "target = 'churn'\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data_no_total[features], data_no_total[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Step 2: Identify categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 3: Define preprocessors for each set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 4: Preprocess the data\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Step 5: Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)  # Encode \"Yes\"/\"No\" as 1/0\n",
    "y_val_encoded = label_encoder.transform(y_val)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 6: Apply SMOTE to the training data\n",
    "ada = ADASYN(random_state=42)\n",
    "X_train_resampled, y_train_resampled = ada.fit_resample(X_train_preprocessed, y_train_encoded)\n",
    "# Verify class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_train_resampled))\n",
    "\n",
    "# Step 7: Define models to evaluate\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 8: Train and evaluate each model on the resampled data\n",
    "model_performance = {}\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "    # Fit model on the SMOTE-resampled training data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict and evaluate on the original validation set\n",
    "    y_val_pred = model.predict(X_val_preprocessed)\n",
    "    accuracy = accuracy_score(y_val_encoded, y_val_pred)\n",
    "    model_performance[model_name] = accuracy\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"\\n{model_name} Classification Report:\")\n",
    "    print(classification_report(y_val_encoded, y_val_pred))\n",
    "    \n",
    "    # Capture feature importances for applicable models\n",
    "    if model_name in ['Random Forest', 'Decision Tree', 'Gradient Boosting']:\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df[model_name] = importances\n",
    "\n",
    "# Add feature names to feature importance DataFrame if applicable\n",
    "if not feature_importance_df.empty:\n",
    "    feature_names = train_preprocessor.get_feature_names_out()\n",
    "    feature_importance_df['Feature'] = feature_names\n",
    "    feature_importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "print(model_performance)\n",
    "\n",
    "if not feature_importance_df.empty:\n",
    "    print(\"\\nFeature Importances Summary:\")\n",
    "    print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64258045-7690-4d5f-9f67-9046936e04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EasyEnsemble Validation Accuracy: 75.320970042796\n",
      "EasyEnsemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.91      0.74      0.82      1037\n",
      "         Yes       0.52      0.80      0.63       365\n",
      "\n",
      "    accuracy                           0.75      1402\n",
      "   macro avg       0.72      0.77      0.72      1402\n",
      "weighted avg       0.81      0.75      0.77      1402\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define and fit the EasyEnsembleClassifier\n",
    "model = EasyEnsembleClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = model.predict(X_val_preprocessed)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"EasyEnsemble Validation Accuracy:\", accuracy_score(y_val, y_val_pred) * 100)\n",
    "print(\"EasyEnsemble Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344af228-bb96-4a7c-9e30-f462cf655fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
