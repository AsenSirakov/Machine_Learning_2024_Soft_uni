{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1c86f-0617-44dc-9ee4-bc67a080a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature columns and target column\n",
    "features = telecom_data_cleaned_no_duplicates.columns.drop('churn')\n",
    "target = 'churn'\n",
    "\n",
    "# Step 1: Split the data into training (60%), validation (20%), and testing (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(telecom_data_cleaned_no_duplicates[features], telecom_data_cleaned_no_duplicates[target], test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Step 2: Define separate transformers for each set\n",
    "# Transformer for training set\n",
    "train_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Transformer for validation set\n",
    "val_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Transformer for test set\n",
    "test_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)])\n",
    "\n",
    "# Step 3: Fit each preprocessor only on its respective set and transform\n",
    "X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "\n",
    "# Display shapes of the preprocessed sets to confirm successful transformation\n",
    "print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "print(\"Test set shape:\", X_test_preprocessed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994885f8-3c61-4feb-b9f5-c8cc39e6888d",
   "metadata": {},
   "source": [
    "16. Let's also encode it and scale it so that it's ready to be run on different models\n",
    "17. Okay now let's continue by preparing the data for machine learning deployment , beggining with one-hot encoding as it would work better than manually mapping the categories to values like 0,1,2 will introduce a FALSE sense of order in places where there ain't no order (\"None\" < \"DSL\" < \"Fiber optic\") which is not the case they are seperate things and do not follow any natural order and the model could assume false relationships.\n",
    "18. Okay now let's continue by preparing the data for machine learning deployment , beggining with one-hot encoding as it would work better than manually mapping the categories to values like 0,1,2 will introduce a FALSE sense of order in places where there ain't no order (\"None\" < \"DSL\" < \"Fiber optic\") which is not the case they are seperate things and do not follow any natural order and the model could assume false relationships.\n",
    "19. For that we will have to convert the categorical columns into dummies which in this case I assume would come with the addition of new features, as the encoding works with (True(1) and False(0))and in some columns we have 3 values.\n",
    "20. Also let's first split the data into train-test and validation so that we don't encode and scale the whole data to prevent data leakage. And we will scale and encode every set seperately in order not to leak any knowledge into the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8e75f-1271-4395-8a4f-b3bbe64e1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_preprocess_data(data_no_monthly_tenure, target_column, test_size=0.4, val_size=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and testing sets, identifies categorical and numerical features,\n",
    "    and preprocesses each set using scaling for numerical features and one-hot encoding for categorical features.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The dataset to split and preprocess.\n",
    "    - target_column (str): The name of the target column.\n",
    "    - test_size (float): The proportion of data to set aside for validation + testing.\n",
    "    - val_size (float): The proportion of the remaining data to use for validation.\n",
    "    - random_state (int): The random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_preprocessed, X_val_preprocessed, X_test_preprocessed: Preprocessed training, validation, and test sets.\n",
    "    - y_train, y_val, y_test: Target values for training, validation, and test sets.\n",
    "    - train_preprocessor: The ColumnTransformer used for the training set (for feature names).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the data\n",
    "    features = data_no_monthly_tenure.columns.drop(target_column)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(data_no_monthly_tenure[features], data_no_monthly_tenure[target_column], test_size=test_size, random_state=random_state)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    # Define categorical and numerical features\n",
    "    categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "    numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Define preprocessors for each set\n",
    "    train_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)]\n",
    "    )\n",
    "    \n",
    "    val_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)]\n",
    "    )\n",
    "    \n",
    "    test_preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(drop='first'), categorical_features)]\n",
    "    )\n",
    "    \n",
    "    # Fit and transform each set\n",
    "    X_train_preprocessed = train_preprocessor.fit_transform(X_train)\n",
    "    X_val_preprocessed = val_preprocessor.fit_transform(X_val)\n",
    "    X_test_preprocessed = test_preprocessor.fit_transform(X_test)\n",
    "    \n",
    "    # Display shapes of the preprocessed sets\n",
    "    print(\"Training set shape:\", X_train_preprocessed.shape)\n",
    "    print(\"Validation set shape:\", X_val_preprocessed.shape)\n",
    "    print(\"Test set shape:\", X_test_preprocessed.shape)\n",
    "    \n",
    "    return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train, y_val, y_test, train_preprocessor\n",
    "\n",
    "# Usage example:\n",
    "# Assuming 'data_no_total' is your DataFrame and 'churn' is your target column\n",
    "X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train, y_val, y_test, train_preprocessor = split_and_preprocess_data(data_no_monthly_tenure, 'churn')\n",
    "\n",
    "# Now you can use train_preprocessor to get feature names\n",
    "feature_names = train_preprocessor.get_feature_names_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
